import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging
from utils.logger import flush_log
from datetime import datetime

logger = logging.getLogger(__name__)


class AdvancedDefectAnalyzer:
    """ê³ ë„í™”ëœ ë¶ˆëŸ‰ ë¶„ì„ ë° ì œì•ˆ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.failure_mode_patterns = {}
        self.risk_factors = {}
        self.improvement_db = self._build_improvement_database()

    def _build_improvement_database(self) -> Dict[str, Dict]:
        """ê°œì„  ë°©ì•ˆ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        return {
            # ê¸°ì¡´ ë¶€í’ˆë“¤
            "CENTER O-RING": {
                "common_causes": ["ê°€ì•• ë¶ˆëŸ‰", "ì‚½ì… ë¶ˆëŸ‰", "ëˆ„ìˆ˜"],
                "specific_actions": [
                    "O-ë§ ê·œê²© ì¬ê²€í†  ë° ê³µì°¨ ê´€ë¦¬ ê°•í™”",
                    "ì¡°ë¦½ ì‹œ O-ë§ ì†ìƒ ë°©ì§€ë¥¼ ìœ„í•œ ì‘ì—…ì êµìœ¡",
                    "ê°€ì•• í…ŒìŠ¤íŠ¸ ì••ë ¥ ë‹¨ê³„ë³„ ì¡°ì •",
                    "O-ë§ ì„¤ì¹˜ ì „ ì²­ê²°ë„ ê²€ì‚¬ ê°•í™”",
                ],
                "inspection_points": ["O-ë§ í‘œë©´ ìƒíƒœ", "ì‚½ì… ê¹Šì´", "ê°€ì•• ì‹œ ëˆ„ìˆ˜ì "],
                "priority_level": "HIGH",
            },
            # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¶€í’ˆë“¤ (ì˜¤ëŠ˜ í•™ìŠµ ë°ì´í„° ë°˜ì˜)
            "SPEED CONTROLLER": {
                "common_causes": [
                    "Speed Controller Leak (92ê±´ - ìµœë‹¤ë°œìƒ)",
                    "PFA ì¬ì§ˆ ëˆ„ìˆ˜ (72ê±´)",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ ëˆ„ìˆ˜ (40ê±´)",
                    "Body ì¬ì§ˆ leak",
                    "He ê°€ì••ê²€ì‚¬ ë¶ˆí•©ê²©",
                ],
                "specific_actions": [
                    "Speed Controller ì „ì²´ êµì²´ (ë¯¸ë³´ì¦ ë¶€í’ˆ ìš°ì„ )",
                    "PFA ì¬ì§ˆ ëˆ„ìˆ˜ ê·¼ë³¸ì›ì¸ ë¶„ì„ ë° ëŒ€ì±… ìˆ˜ë¦½",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ í’ˆì§ˆ ê¸°ì¤€ ê°•í™”",
                    "ê°€ì••ê²€ì‚¬ ì••ë ¥ ë° ì‹œê°„ ìµœì í™”",
                    "ê³µê¸‰ì—…ì²´ í’ˆì§ˆ ê´€ë¦¬",
                ],
                "inspection_points": [
                    "Speed Controller Leak í…ŒìŠ¤íŠ¸ (í•„ìˆ˜)",
                    "PFA ì¬ì§ˆ ë‚´ì•• ì„±ëŠ¥",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ ë°€ì°©ë„",
                    "Body ì¬ì§ˆ crack ê²€ì‚¬",
                    "He leak í…ŒìŠ¤íŠ¸ ê¸°ì¤€ ì¤€ìˆ˜",
                ],
                "priority_level": "CRITICAL",  # ìµœë‹¤ë°œìƒìœ¼ë¡œ CRITICAL ìƒí–¥
                "enhanced_keywords": ["Speed Controller Leak", "PFA", "ìš°ë ˆíƒ„", "Body", "LEAK"],
                "defect_rate_trend": "ì¦ê°€ (ì „ì²´ 18.3% ì°¨ì§€)",
            },
            "HEATING JACKET": {
                "common_causes": ["ì˜¨ë„ ì œì–´ ë¶ˆëŸ‰", "ì ˆì—° ì†ìƒ", "íˆí„° ì†Œì†"],
                "specific_actions": [
                    "ì˜¨ë„ ì„¼ì„œ êµì • ë° ì ê²€",
                    "íˆí„° ì €í•­ê°’ ì¸¡ì • ë° êµì²´",
                    "ì ˆì—° ìƒíƒœ ì ê²€ ê°•í™”",
                    "ì˜¨ë„ ì œì–´ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”",
                ],
                "inspection_points": ["ì˜¨ë„ ì •í™•ë„", "ì ˆì—° ì €í•­", "íˆí„° ìƒíƒœ"],
                "priority_level": "HIGH",
            },
            "LEAK SENSOR": {
                "common_causes": ["ì„¼ì„œ ê°ë„ ë¶ˆëŸ‰", "ì˜¤ì—¼", "ì‹ í˜¸ ë…¸ì´ì¦ˆ"],
                "specific_actions": [
                    "ì„¼ì„œ ê°ë„ ì¬ì¡°ì •",
                    "ì„¼ì„œ ì²­ì†Œ ë° ë³´í˜¸ ê°•í™”",
                    "ì‹ í˜¸ í•„í„°ë§ ê°œì„ ",
                    "ì„¼ì„œ ìœ„ì¹˜ ìµœì í™”",
                ],
                "inspection_points": ["ê°ë„ ì„¤ì •", "ì„¼ì„œ ì²­ê²°ë„", "ì‹ í˜¸ í’ˆì§ˆ"],
                "priority_level": "MEDIUM",
            },
            "TOUCH SCREEN": {
                "common_causes": ["í„°ì¹˜ ê°ë„ ë¶ˆëŸ‰", "í™”ë©´ ì†ìƒ", "í†µì‹  ì˜¤ë¥˜"],
                "specific_actions": [
                    "í„°ì¹˜ ìŠ¤í¬ë¦° ìº˜ë¦¬ë¸Œë ˆì´ì…˜",
                    "í™”ë©´ ë³´í˜¸ í•„ë¦„ ì ê²€",
                    "í†µì‹  ì¼€ì´ë¸” ì—°ê²° ìƒíƒœ í™•ì¸",
                    "HMI ì†Œí”„íŠ¸ì›¨ì–´ ì—…ë°ì´íŠ¸",
                ],
                "inspection_points": ["í„°ì¹˜ ë°˜ì‘", "í™”ë©´ ìƒíƒœ", "í†µì‹  ì—°ê²°"],
                "priority_level": "MEDIUM",
            },
            "FEMALE CONNECTOR": {
                "common_causes": ["ì ‘ì´‰ ë¶ˆëŸ‰", "ì‚½ì… ë¶ˆëŸ‰", "ë¶€ì‹"],
                "specific_actions": [
                    "ì»¤ë„¥í„° í•€ ì ‘ì´‰ ì••ë ¥ ì¡°ì •",
                    "ì‚½ì… ê°€ì´ë“œ ì •ë ¬ ì ê²€",
                    "ë°©ì²­ ì²˜ë¦¬ ë° ë³´ê´€ í™˜ê²½ ê°œì„ ",
                    "ì»¤ë„¥í„° í•˜ìš°ì§• êµì²´",
                ],
                "inspection_points": ["ì ‘ì´‰ ì €í•­", "ì‚½ì…ë ¥", "ë¶€ì‹ ìƒíƒœ"],
                "priority_level": "MEDIUM",
            },
            "REDUCER DOUBLE Y UNION": {
                "common_causes": [
                    "ì‚½ì…ë¶€ ë¶ˆëŸ‰ (41ê±´ - ìµœë‹¤ í‚¤ì›Œë“œ)", 
                    "N2 REDUCING DOUBLE Y UNION Leak (12ê±´)",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ ë¬¸ì œ",
                    "ì²´ê²° ë¶ˆëŸ‰", 
                    "ëˆ„ì„¤"
                ],
                "specific_actions": [
                    "ì‚½ì…ë¶€ ì„¤ê³„ ë° ê°€ê³µ ì •ë°€ë„ ê°œì„ ",
                    "N2 REDUCING DOUBLE Y UNION ëˆ„ìˆ˜ ë°©ì§€ ëŒ€ì±…",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ í’ˆì§ˆ ê¸°ì¤€ ê°•í™”",
                    "ì²´ê²° í† í¬ í‘œì¤€í™”",
                    "ì¡°ë¦½ ìˆœì„œ ë° ë°©ë²• í‘œì¤€í™”",
                ],
                "inspection_points": [
                    "ì‚½ì…ë¶€ ì¹˜ìˆ˜ ì •ë°€ë„ (í•„ìˆ˜)",
                    "N2 REDUCING Leak í…ŒìŠ¤íŠ¸",
                    "ìš°ë ˆíƒ„ ì¬ì§ˆ ìƒíƒœ ì ê²€",
                    "ì²´ê²° í† í¬ ì¸¡ì •",
                ],
                "priority_level": "HIGH",  # 28ê±´ìœ¼ë¡œ HIGH ìƒí–¥
                "enhanced_keywords": ["ì‚½ì…ë¶€", "N2 REDUCING DOUBLE Y UNION", "Leak", "ìš°ë ˆíƒ„", "LEAK"],
                "defect_rate_trend": "ì¤‘ê°„ (ì „ì²´ 5.2% ì°¨ì§€)",
            },
            "O-RING": {
                "common_causes": [
                    "Ring ë³€í˜• (21ê±´)",
                    "ì¡°ë¦½ ë¶ˆëŸ‰ (15ê±´)",
                    "Ring ì¬ì§ˆ ë¬¸ì œ",
                    "ê°€ì•• ë¶ˆëŸ‰",
                    "ì‚½ì… ë¶ˆëŸ‰"
                ],
                "specific_actions": [
                    "O-Ring ë³€í˜• ë°©ì§€ ì·¨ê¸‰ ì§€ì¹¨ ìˆ˜ë¦½",
                    "ì¡°ë¦½ ê³µì • í‘œì¤€í™” ë° ì‘ì—…ì êµìœ¡",
                    "O-Ring ì¬ì§ˆ ë° ê·œê²© ì¬ê²€í† ",
                    "ê°€ì•• í…ŒìŠ¤íŠ¸ ì¡°ê±´ ìµœì í™”",
                    "ì‚½ì… ì‹œ ì†ìƒ ë°©ì§€ ë„êµ¬ ê°œë°œ",
                ],
                "inspection_points": [
                    "O-Ring ë³€í˜• ìƒíƒœ ê²€ì‚¬",
                    "ì¡°ë¦½ ì •í™•ì„± í™•ì¸",
                    "Ring ì¬ì§ˆ ê·œê²© ì í•©ì„±",
                    "ê°€ì•• ëˆ„ì„¤ í…ŒìŠ¤íŠ¸",
                ],
                "priority_level": "MEDIUM",
                "enhanced_keywords": ["Ring", "ì¡°ë¦½", "ë¶ˆëŸ‰", "ë³€í˜•", "RING"],
                "defect_rate_trend": "ë‚®ìŒ (ì „ì²´ 3.5% ì°¨ì§€)",
            },
            "UNION TEE": {
                "common_causes": ["ì²´ê²° ë¶ˆëŸ‰", "ë‚˜ì‚¬ì‚° ë¶ˆëŸ‰", "ë°€ì°© ë¶ˆëŸ‰"],
                "specific_actions": [
                    "ë‚˜ì‚¬ì‚° ê·œê²© ë° ì²´ê²° í† í¬ í‘œì¤€í™”",
                    "ì²´ê²° ìˆœì„œ ë° ë°©ë²• ì‘ì—…ì§€ì¹¨ì„œ ì¬ì •ë¹„",
                    "ìœ ë‹ˆì˜¨í‹° ê°€ê³µ ì •ë°€ë„ í–¥ìƒ",
                    "ë°€ì°©ë©´ ì²­ì†Œ ë° ì‹¤ë§ ì¬ë£Œ ê²€í† ",
                ],
                "inspection_points": ["ë‚˜ì‚¬ì‚° ìƒíƒœ", "ì²´ê²° í† í¬", "ë°€ì°©ë©´ í‰í™œë„"],
                "priority_level": "MEDIUM",
            },
            "HEATING PAB PIPE": {
                "common_causes": ["ìš©ì ‘ ë¶ˆëŸ‰", "ì—´ë³€í˜•", "ì¬ì§ˆ ë¶ˆëŸ‰"],
                "specific_actions": [
                    "ìš©ì ‘ ì¡°ê±´ ìµœì í™” ë° ì‘ì—…ì ê¸°ëŠ¥ í–¥ìƒ",
                    "ì—´ì²˜ë¦¬ ê³µì • ì˜¨ë„ ë° ì‹œê°„ ì¬ê²€í† ",
                    "íŒŒì´í”„ ì¬ì§ˆ ê·œê²© ê²€ì¦",
                    "ìš©ì ‘ í›„ ë¹„íŒŒê´´ê²€ì‚¬ ê°•í™”",
                ],
                "inspection_points": ["ìš©ì ‘ í’ˆì§ˆ", "ì¹˜ìˆ˜ ì •ë°€ë„", "ë‚´ì•• ì„±ëŠ¥"],
                "priority_level": "HIGH",
            },
            "MALE ADAPTER": {
                "common_causes": ["ê°€ê³µ ì •ë°€ë„", "ì‚½ì… ë¶ˆëŸ‰", "ì²´ê²° ë¶ˆëŸ‰"],
                "specific_actions": [
                    "ê°€ê³µ ì¹˜ìˆ˜ ì •ë°€ë„ í–¥ìƒ ë° ê²€ì‚¬ ê¸°ì¤€ ê°•í™”",
                    "ì‚½ì… ì‹œ ì •ë ¬ ê°€ì´ë“œ ë„êµ¬ ê°œë°œ",
                    "ì–´ëŒ‘í„° í‘œë©´ ì²˜ë¦¬ ê°œì„ ",
                    "ì¡°ë¦½ ê³µì • í‘œì¤€í™”",
                ],
                "inspection_points": ["ì¹˜ìˆ˜ ì •ë°€ë„", "í‘œë©´ ì¡°ë„", "ì‚½ì… ì €í•­"],
                "priority_level": "MEDIUM",
            },
            "MALE CONNECTOR": {
                "common_causes": [
                    "MALE CONNECTOR Fitting nut Water Leak (17ê±´)",
                    "N2 Male Connector Teflon ë¶€ì¡± (ë¶€í’ˆëˆ„ë½)",
                    "PCW Flow Sensor ê´€ë ¨ ë¶ˆëŸ‰",
                    "Fitting ì²´ê²° ë¶ˆëŸ‰",
                    "Teflon ì‘ì—… ë¶ˆëŸ‰",
                ],
                "specific_actions": [
                    "MALE CONNECTOR Fitting nut ëˆ„ìˆ˜ ê·¼ë³¸ì›ì¸ ë¶„ì„",
                    "Teflon ë¶€ì¡± í˜„ìƒ ë°©ì§€ë¥¼ ìœ„í•œ ì¬ê³  ê´€ë¦¬ ê°•í™”",
                    "PCW Flow Sensor ì—°ê²°ë¶€ ê²€ì‚¬ í‘œì¤€í™”",
                    "Fitting nut ì²´ê²° í† í¬ í‘œì¤€í™”",
                    "Teflon í…Œì´í”„ ê°ê¸° ì‘ì—… ì§€ì¹¨ì„œ ê°œì •",
                ],
                "inspection_points": [
                    "MALE CONNECTOR Water Leak í…ŒìŠ¤íŠ¸",
                    "Teflon ì¬ë£Œ ì¶©ë¶„ì„± í™•ì¸",
                    "PCW Flow Sensor ì—°ê²° ìƒíƒœ",
                    "Fitting nut ì²´ê²° í† í¬",
                ],
                "priority_level": "HIGH",  # 40ê±´ìœ¼ë¡œ HIGH ìƒí–¥
                "enhanced_keywords": ["MALE CONNECTOR Fitting nut Water Leak", "ë¶€ì¡±", "N2 Male Connector Teflon", "PCW Flow Sensor"],
                "defect_rate_trend": "ì¤‘ê°„ (ì „ì²´ 7.4% ì°¨ì§€)",
            },
            # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë¶€í’ˆë“¤ ì¶”ê°€
            "BURNER SCRAPER LINE-01": {
                "common_causes": ["Pipe Tee ë°°ê´€ Crack", "Leak ë°œìƒ"],
                "specific_actions": [
                    "Pipe Tee ë°°ê´€ êµì²´",
                    "Crack ë°œìƒ ì›ì¸ ë¶„ì„",
                    "ë°°ê´€ ì¬ì§ˆ ê²€í† ",
                    "ì„¤ì¹˜ ê³µì • ê°œì„ ",
                ],
                "inspection_points": ["ë°°ê´€ Crack ìƒíƒœ", "Leak í…ŒìŠ¤íŠ¸", "ë°°ê´€ ì¬ì§ˆ"],
                "priority_level": "HIGH",
            },
            "MALE ELBOW": {
                "common_causes": [
                    "MALE ELBOW Fitting nut Water Leak (40ê±´ - ë†’ì€ ë¹ˆë„)",
                    "BCW Jaco Fitting Nut Leak (10ê±´)",
                    "TEFLON ë¶€ì¡± (15ê±´)",
                    "ì²´ê²° ë¶ˆëŸ‰",
                    "ë¶€í’ˆ ëˆ„ë½",
                ],
                "specific_actions": [
                    "MALE ELBOW Fitting nut ëˆ„ìˆ˜ íŒ¨í„´ ë¶„ì„ ë° ëŒ€ì±…",
                    "BCW Jaco Fitting Nut êµì²´ ê¸°ì¤€ ìˆ˜ë¦½",
                    "TEFLON ì¬ë£Œ ì¶©ë¶„ì„± í™•ë³´ ë°©ì•ˆ",
                    "ì²´ê²° í† í¬ í‘œì¤€í™” ë° êµìœ¡ ê°•í™”",
                    "ë¶€í’ˆ ëˆ„ë½ ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì ìš©",
                ],
                "inspection_points": [
                    "MALE ELBOW Water Leak í…ŒìŠ¤íŠ¸",
                    "BCW Jaco Fitting Nut ìƒíƒœ ì ê²€",
                    "TEFLON ì¬ë£Œ ë³´ìœ ëŸ‰ í™•ì¸",
                    "ì²´ê²° í† í¬ ì¸¡ì •",
                ],
                "priority_level": "HIGH",  # 62ê±´ìœ¼ë¡œ HIGH ìƒí–¥
                "enhanced_keywords": ["MALE ELBOW Fitting nut Water Leak", "TEFLON", "BCW Jaco Fitting Nut Leak", "ELBOW", "ë¶€ì¡±"],
                "defect_rate_trend": "ë†’ìŒ (ì „ì²´ 11.5% ì°¨ì§€)",
            },
            "UNION ELBOW": {
                "common_causes": [
                    "ì²´ê²° ë¶ˆëŸ‰ (ì£¼ìš” ì›ì¸)",
                    "Leak ë°œìƒ",
                    "ëˆ„ë½",
                    "Fitting Nut ì²´ê²°ë¶ˆëŸ‰",
                    "Tube ì‚½ì… ë¶ˆëŸ‰",
                ],
                "specific_actions": [
                    "ì²´ê²° ë¶ˆëŸ‰ ê·¼ë³¸ì›ì¸ ë¶„ì„",
                    "Leak ë°©ì§€ ëŒ€ì±… ìˆ˜ë¦½",
                    "ë¶€í’ˆ ëˆ„ë½ ë°©ì§€ ì‹œìŠ¤í…œ êµ¬ì¶•",
                    "Fitting Nut ì²´ê²° í† í¬ í‘œì¤€í™”",
                    "ì²´ê²° ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸ ê°œì„ ",
                ],
                "inspection_points": [
                    "ì²´ê²° ìƒíƒœ ì „ìˆ˜ ê²€ì‚¬",
                    "Leak í…ŒìŠ¤íŠ¸ ê°•í™”",
                    "ë¶€í’ˆ ëˆ„ë½ ê²€ì‚¬",
                    "Fitting Nut ì²´ê²° í† í¬",
                ],
                "priority_level": "MEDIUM",
                "enhanced_keywords": ["ì²´ê²°", "ë¶ˆëŸ‰", "UNION ELBOW", "Leak", "ëˆ„ë½"],
            },
            # ë¯¸ë¶„ë¥˜ ë¶€í’ˆì„ ìœ„í•œ ì¼ë°˜ì ì¸ ì œì•ˆ
            "ë¯¸ë¶„ë¥˜": {
                "common_causes": [
                    "ë¶€í’ˆëª… ëˆ„ë½/ì˜¤ê¸°ì…",
                    "ì¡°ë¦½ë„ë©´ ì •ë³´ ë¶ˆì¼ì¹˜",
                    "ì‘ì—…ì§€ì‹œì„œ ë¯¸ë¹„",
                    "ë¶€í’ˆ ì½”ë“œ ì²´ê³„ ë¯¸ì •ë¦½",
                    "ê²€ì‚¬ ê¸°ì¤€ ë¶ˆëª…í™•",
                ],
                "specific_actions": [
                    "ë¶€í’ˆ ì‹ë³„ ë¼ë²¨ë§ ì‹œìŠ¤í…œ êµ¬ì¶•",
                    "ì¡°ë¦½ë„ë©´ ë° BOM ì •í™•ì„± ê²€ì¦",
                    "ì‘ì—… ì§€ì‹œì„œ ë° ì²´í¬ë¦¬ìŠ¤íŠ¸ í‘œì¤€í™”",
                    "ë¶€í’ˆ ì½”ë“œ ì²´ê³„ ì¬ì •ë¹„ ë° êµìœ¡",
                    "ê²€ì‚¬ ê¸°ì¤€ì„œ ëª…í™•í™”",
                    "ì‘ì—…ì êµìœ¡ ê°•í™” (ë¶€í’ˆ ì‹ë³„ë²•)",
                    "í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ ê°œì„ ",
                ],
                "inspection_points": [
                    "ë¶€í’ˆ ë¼ë²¨ ë¶€ì°© ìƒíƒœ",
                    "ì‘ì—… ì§€ì‹œì„œ ì™„ì„±ë„",
                    "ê²€ì‚¬ ê¸°ì¤€ì„œ ëª…í™•ì„±",
                    "ì‘ì—…ì ë¶€í’ˆ ì‹ë³„ ëŠ¥ë ¥",
                ],
                "priority_level": "HIGH",  # ë¯¸ë¶„ë¥˜ëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ë³€ê²½
                "detailed_analysis": {
                    "main_keywords": [
                        "ì¡°ë¦½ë„",
                        "ì¡°ë¦½",
                        "ë°©í–¥",
                        "ë°˜ëŒ€",
                        "ì ìš©",
                        "BOM",
                        "í‘œê¸°",
                        "ì˜¤ë¥˜",
                    ],
                    "likely_root_cause": "ì¡°ë¦½ ë„ë©´ ë° ì‘ì—… ì§€ì‹œì„œì˜ ì •ë³´ ë¶ˆì¼ì¹˜",
                    "impact_assessment": "ì „ì²´ ë¶ˆëŸ‰ì˜ 26.7% ì°¨ì§€, í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„ ê·¼ë³¸ì  ë¬¸ì œ",
                },
            },
        }

    def advanced_failure_analysis(
        self, data: pd.DataFrame, predictions: List[Dict]
    ) -> Dict:
        """ê³ ë„í™”ëœ ë¶ˆëŸ‰ ë¶„ì„"""
        logger.info("ğŸ”¬ ê³ ë„í™”ëœ ì‹¤íŒ¨ ë¶„ì„ ì‹œì‘...")

        # 1. ì‹¤íŒ¨ ëª¨ë“œ íŒ¨í„´ ë¶„ì„
        failure_patterns = self._analyze_failure_patterns(data)

        # 2. ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = self._analyze_correlations(data)

        # 3. ì˜ˆì¸¡ ë¶ˆëŸ‰ë¥  ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„
        risk_analysis = self._analyze_risk_levels(predictions)

        # 4. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = self._analyze_trends(data)

        result = {
            "failure_patterns": failure_patterns,
            "correlations": correlations,
            "risk_analysis": risk_analysis,
            "trend_analysis": trend_analysis,
        }

        logger.info("âœ… ê³ ë„í™”ëœ ì‹¤íŒ¨ ë¶„ì„ ì™„ë£Œ")
        return result

    def _analyze_failure_patterns(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì‹¤íŒ¨ íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        if len(data) == 0:
            return {}

        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì•ˆì „í•œ ë¶„ì„
        required_cols = ["ì œí’ˆëª…", "ë¶€í’ˆëª…"]
        available_cols = [col for col in required_cols if col in data.columns]

        if "ê²€ì¶œë‹¨ê³„" in data.columns:
            available_cols.append("ê²€ì¶œë‹¨ê³„")

        if len(available_cols) < 2:
            logger.warning(f"ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {available_cols}")
            return {}

        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ìœ¼ë¡œ íŒ¨í„´ ë¶„ì„
        patterns = data.groupby(available_cols).size().reset_index(name="count")
        patterns = patterns.sort_values("count", ascending=False)

        # ìƒìœ„ íŒ¨í„´ë“¤ì˜ íŠ¹ì§• ë¶„ì„
        top_patterns = patterns.head(10)
        pattern_analysis = []

        for _, row in top_patterns.iterrows():
            product = row.get("ì œí’ˆëª…", "Unknown")
            stage = row.get("ê²€ì¶œë‹¨ê³„", "Unknown")
            part = row.get("ë¶€í’ˆëª…", "Unknown")

            # NaN ê°’ ì²˜ë¦¬
            if pd.isna(product) or str(product) == "nan":
                product = "ë¯¸ë¶„ë¥˜"
            if pd.isna(stage) or str(stage) == "nan":
                stage = "ë¯¸ë¶„ë¥˜"
            if pd.isna(part) or str(part) == "nan":
                part = "ë¯¸ë¶„ë¥˜"

            count = row["count"]

            # í•´ë‹¹ íŒ¨í„´ì˜ ìƒì„¸ ë¶„ì„ (ì•ˆì „í•œ í•„í„°ë§)
            pattern_filter = (
                (data["ì œí’ˆëª…"] == product) if "ì œí’ˆëª…" in data.columns else True
            )
            if "ê²€ì¶œë‹¨ê³„" in data.columns and stage != "Unknown":
                pattern_filter = pattern_filter & (data["ê²€ì¶œë‹¨ê³„"] == stage)
            if "ë¶€í’ˆëª…" in data.columns:
                pattern_filter = pattern_filter & (data["ë¶€í’ˆëª…"] == part)

            pattern_data = (
                data[pattern_filter] if isinstance(pattern_filter, pd.Series) else data
            )

            # í‚¤ì›Œë“œ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
            keywords = []
            if "ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©" in pattern_data.columns:
                for content in pattern_data["ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©"]:
                    if pd.notna(content):
                        keywords.extend(str(content).split())

            keyword_freq = Counter(keywords).most_common(5) if keywords else []

            pattern_analysis.append(
                {
                    "pattern": f"{product}_{stage}_{part}",
                    "frequency": count,
                    "percentage": (count / len(data)) * 100,
                    "keywords": keyword_freq,
                    "risk_level": self._calculate_pattern_risk(count, len(data)),
                }
            )

        return {
            "top_patterns": pattern_analysis,
            "total_patterns": len(patterns),
            "concentration_index": self._calculate_concentration_index(patterns),
        }

    def _analyze_correlations(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ìš”ì¸ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        if len(data) == 0:
            return {}

        correlations = {}

        # ì œí’ˆëª…ë³„ ë¶ˆëŸ‰ ë¶€í’ˆ ìƒê´€ê´€ê³„ (ì•ˆì „í•˜ê²Œ)
        if "ì œí’ˆëª…" in data.columns and "ë¶€í’ˆëª…" in data.columns:
            try:
                product_part_corr = (
                    data.groupby(["ì œí’ˆëª…", "ë¶€í’ˆëª…"]).size().unstack(fill_value=0)
                )
                if len(product_part_corr) > 1 and len(product_part_corr.columns) > 1:
                    correlations["product_part"] = (
                        product_part_corr.corr().abs().mean().mean()
                    )
            except Exception as e:
                logger.warning(f"ì œí’ˆëª…-ë¶€í’ˆëª… ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

        # ê²€ì¶œë‹¨ê³„ë³„ ë¶€í’ˆ ìƒê´€ê´€ê³„ (ì•ˆì „í•˜ê²Œ)
        if "ê²€ì¶œë‹¨ê³„" in data.columns and "ë¶€í’ˆëª…" in data.columns:
            try:
                stage_part_corr = (
                    data.groupby(["ê²€ì¶œë‹¨ê³„", "ë¶€í’ˆëª…"]).size().unstack(fill_value=0)
                )
                if len(stage_part_corr) > 1 and len(stage_part_corr.columns) > 1:
                    correlations["stage_part"] = (
                        stage_part_corr.corr().abs().mean().mean()
                    )
            except Exception as e:
                logger.warning(f"ê²€ì¶œë‹¨ê³„-ë¶€í’ˆëª… ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

        return correlations

    def _analyze_risk_levels(self, predictions: List[Dict]) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„"""
        if not predictions:
            return {}

        risk_levels = []
        for pred in predictions:
            defect_rate = pred.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0)
            part = pred.get("ë¶€í’ˆ", "Unknown")

            if defect_rate >= 15:
                risk_level = "CRITICAL"
            elif defect_rate >= 5:
                risk_level = "HIGH"
            elif defect_rate >= 1:
                risk_level = "MEDIUM"
            else:
                risk_level = "LOW"

            risk_levels.append(
                {"part": part, "defect_rate": defect_rate, "risk_level": risk_level}
            )

        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_distribution = Counter([r["risk_level"] for r in risk_levels])

        return {
            "risk_levels": risk_levels,
            "distribution": dict(risk_distribution),
            "critical_parts": [
                r["part"] for r in risk_levels if r["risk_level"] == "CRITICAL"
            ],
        }

    def _analyze_trends(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(data) == 0 or "ë‚ ì§œ" not in data.columns:
            return {}

        # ìµœê·¼ 30ì¼ê°„ íŠ¸ë Œë“œ
        recent_data = data.tail(
            30
        )  # ìµœê·¼ 30ê±´ (ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ìµœê·¼ ë°ì´í„°ë¡œ ëŒ€ì²´)

        # ë¶€í’ˆë³„ íŠ¸ë Œë“œ (ì•ˆì „í•˜ê²Œ)
        part_trends = (
            recent_data.groupby("ë¶€í’ˆëª…").size().sort_values(ascending=False)
            if "ë¶€í’ˆëª…" in recent_data.columns
            else pd.Series()
        )

        # ì œí’ˆë³„ íŠ¸ë Œë“œ (ì•ˆì „í•˜ê²Œ)
        product_trends = (
            recent_data.groupby("ì œí’ˆëª…").size().sort_values(ascending=False)
            if "ì œí’ˆëª…" in recent_data.columns
            else pd.Series()
        )

        return {
            "recent_part_trends": dict(part_trends.head(5)),
            "recent_product_trends": dict(product_trends.head(5)),
            "trend_direction": self._calculate_trend_direction(recent_data),
        }

    def _calculate_pattern_risk(self, count: int, total: int) -> str:
        """íŒ¨í„´ë³„ ìœ„í—˜ë„ ê³„ì‚°"""
        percentage = (count / total) * 100
        if percentage >= 5:
            return "HIGH"
        elif percentage >= 2:
            return "MEDIUM"
        else:
            return "LOW"

    def _calculate_concentration_index(self, patterns: pd.DataFrame) -> float:
        """ë¶ˆëŸ‰ ì§‘ì¤‘ë„ ì§€ìˆ˜ ê³„ì‚° (í—ˆí•€ë‹¬ ì§€ìˆ˜)"""
        if len(patterns) == 0:
            return 0
        total = patterns["count"].sum()
        shares = (patterns["count"] / total) ** 2
        return shares.sum()

    def _calculate_trend_direction(self, data: pd.DataFrame) -> str:
        """íŠ¸ë Œë“œ ë°©í–¥ ê³„ì‚°"""
        if len(data) < 10:
            return "INSUFFICIENT_DATA"

        # ìµœê·¼ ì ˆë°˜ê³¼ ì´ì „ ì ˆë°˜ ë¹„êµ
        mid_point = len(data) // 2
        recent_avg = len(data[mid_point:]) / (len(data) - mid_point)
        previous_avg = len(data[:mid_point]) / mid_point

        if recent_avg > previous_avg * 1.1:
            return "INCREASING"
        elif recent_avg < previous_avg * 0.9:
            return "DECREASING"
        else:
            return "STABLE"

    def generate_advanced_suggestions(
        self, analysis_results: Dict, predictions: List[Dict]
    ) -> List[Dict]:
        """ê³ ë„í™”ëœ ê°œì„  ì œì•ˆ ìƒì„±"""
        logger.info("ğŸ’¡ Pin Point ê°œì„  ì œì•ˆ ìƒì„± ì¤‘...")
        logger.info(f"ğŸ” ì˜ˆì¸¡ ë°ì´í„°: {len(predictions)}ê°œ")
        logger.info(f"ğŸ” ë¶„ì„ ê²°ê³¼ í‚¤: {list(analysis_results.keys())}")

        suggestions = []

        # 1. ì˜ˆì¸¡ ê¸°ë°˜ ê°œë³„ ë¶€í’ˆ ì œì•ˆ
        for i, pred in enumerate(predictions):
            part = pred.get("ë¶€í’ˆ", "")
            defect_rate = pred.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0)
            model = pred.get("ëª¨ë¸", "")

            logger.info(
                f"ğŸ” ì˜ˆì¸¡ {i+1}: ëª¨ë¸={model}, ë¶€í’ˆ={part}, ë¶ˆëŸ‰ë¥ ={defect_rate}"
            )
            logger.info(
                f"ğŸ” ë¶€í’ˆ '{part}'ì´ DBì— ìˆëŠ”ê°€? {part in self.improvement_db}"
            )
            logger.info(f"ğŸ” DB í‚¤ë“¤: {list(self.improvement_db.keys())}")

            if part in self.improvement_db:
                part_db = self.improvement_db[part]

                # ë¶ˆëŸ‰ë¥ ì— ë”°ë¥¸ ì œì•ˆ ìš°ì„ ìˆœìœ„ ê²°ì •
                if defect_rate >= 15:
                    actions = part_db["specific_actions"][:2]  # ìƒìœ„ 2ê°œ ì¡°ì¹˜
                    urgency = "IMMEDIATE"
                elif defect_rate >= 5:
                    actions = part_db["specific_actions"][:3]  # ìƒìœ„ 3ê°œ ì¡°ì¹˜
                    urgency = "URGENT"
                else:
                    actions = part_db["specific_actions"][:1]  # ìƒìœ„ 1ê°œ ì¡°ì¹˜
                    urgency = "NORMAL"

                suggestion = {
                    "target": f"{model} - {part}",
                    "defect_rate": defect_rate,
                    "urgency": urgency,
                    "root_causes": part_db["common_causes"],
                    "specific_actions": actions,
                    "inspection_points": part_db["inspection_points"],
                    "expected_improvement": self._calculate_expected_improvement(
                        defect_rate, urgency
                    ),
                    "implementation_cost": self._estimate_cost(urgency, len(actions)),
                    "timeline": self._estimate_timeline(urgency),
                }
                suggestions.append(suggestion)

        # 2. íŒ¨í„´ ê¸°ë°˜ ì‹œìŠ¤í…œ ê°œì„  ì œì•ˆ
        if "failure_patterns" in analysis_results:
            pattern_suggestions = self._generate_pattern_based_suggestions(
                analysis_results["failure_patterns"]
            )
            suggestions.extend(pattern_suggestions)

        # 3. ìš°ì„ ìˆœìœ„ ì •ë ¬
        suggestions.sort(
            key=lambda x: (
                {"IMMEDIATE": 0, "URGENT": 1, "NORMAL": 2}[x["urgency"]],
                -x["defect_rate"],
            )
        )

        logger.info(f"âœ… Pin Point ì œì•ˆ ìƒì„± ì™„ë£Œ: {len(suggestions)}ê°œ")
        return suggestions[:5]  # ìƒìœ„ 5ê°œ ì œì•ˆë§Œ ë°˜í™˜

    def _generate_pattern_based_suggestions(
        self, pattern_analysis: Dict
    ) -> List[Dict[str, Any]]:
        """íŒ¨í„´ ê¸°ë°˜ ì‹œìŠ¤í…œ ê°œì„  ì œì•ˆ"""
        suggestions = []

        if "top_patterns" not in pattern_analysis:
            return suggestions

        # ì§‘ì¤‘ë„ê°€ ë†’ì€ íŒ¨í„´ì— ëŒ€í•œ ì‹œìŠ¤í…œ ê°œì„  ì œì•ˆ
        concentration = pattern_analysis.get("concentration_index", 0)

        if concentration > 0.3:  # ë†’ì€ ì§‘ì¤‘ë„
            suggestions.append(
                {
                    "target": "SYSTEM_WIDE",
                    "defect_rate": 0,
                    "urgency": "URGENT",
                    "root_causes": ["íŠ¹ì • íŒ¨í„´ì— ë¶ˆëŸ‰ ì§‘ì¤‘"],
                    "specific_actions": [
                        "ì§‘ì¤‘ ë°œìƒ íŒ¨í„´ì˜ ê³µí†µ ì›ì¸ ì‹¬ì¸µ ë¶„ì„",
                        "í•´ë‹¹ ì œí’ˆêµ°ì˜ ì„¤ê³„ ë° ê³µì • ì „ë©´ ì¬ê²€í† ",
                        "ì‘ì—…ì êµìœ¡ ë° ì¥ë¹„ ì ê²€ ê°•í™”",
                    ],
                    "inspection_points": ["ê³µì • íë¦„ë„", "ì‘ì—… í‘œì¤€ì„œ", "ì¥ë¹„ ìƒíƒœ"],
                    "expected_improvement": "ì „ì²´ ë¶ˆëŸ‰ë¥  20-30% ê°ì†Œ",
                    "implementation_cost": "HIGH",
                    "timeline": "2-3ê°œì›”",
                }
            )

        return suggestions

    def _calculate_expected_improvement(self, current_rate: float, urgency: str) -> str:
        """ì˜ˆìƒ ê°œì„  íš¨ê³¼ ê³„ì‚°"""
        if urgency == "IMMEDIATE":
            improvement = min(50, current_rate * 0.7)
        elif urgency == "URGENT":
            improvement = min(30, current_rate * 0.5)
        else:
            improvement = min(20, current_rate * 0.3)

        return f"ë¶ˆëŸ‰ë¥  {improvement:.1f}%p ê°ì†Œ ì˜ˆìƒ"

    def _estimate_cost(self, urgency: str, action_count: int) -> str:
        """êµ¬í˜„ ë¹„ìš© ì¶”ì •"""
        base_cost = {"IMMEDIATE": "HIGH", "URGENT": "MEDIUM", "NORMAL": "LOW"}
        return base_cost[urgency]

    def _estimate_timeline(self, urgency: str) -> str:
        """ê¸´ê¸‰ë„ ê¸°ë°˜ íƒ€ì„ë¼ì¸ ì¶”ì •"""
        if urgency == "IMMEDIATE":
            return "1ì£¼ ì´ë‚´"
        if urgency == "URGENT":
            return "2-4ì£¼"
        return "1-2ê°œì›”"

    def _get_dynamic_defect_type_mapping(self) -> Dict[str, str]:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë™ì  ë¶ˆëŸ‰ìœ í˜• ë§¤í•‘"""
        try:
            # Teams ë°ì´í„°ì—ì„œ ì‹¤ì œ ë¶ˆëŸ‰ìœ í˜• í™•ì¸
            from data.teams_loader import TeamsDataLoader

            loader = TeamsDataLoader()
            df = loader.load_defect_data_from_teams()

            # ë¶€í’ˆë³„ ê°€ì¥ ë§ì€ ë¶ˆëŸ‰ìœ í˜• ì¶”ì¶œ
            defect_type_mapping = {}

            # ì£¼ìš” ë¶€í’ˆë“¤ì˜ ì‹¤ì œ ë¶ˆëŸ‰ìœ í˜• ë¶„ì„
            parts_to_analyze = [
                "SPEED CONTROLLER",
                "LEAK SENSOR",
                "TOUCH SCREEN",
                "FEMALE CONNECTOR",
                "MALE CONNECTOR",
                "HEATING JACKET",
                "UNION ELBOW",
                "BURNER SCRAPER LINE-01",
                "REDUCER DOUBLE Y UNION",
                "UNEQUAL UNION Y",
                "CLAMP",
                "MALE ELBOW",
                "BULKHEAD UNION",
                "KEY OPERATION VALVE",
                "PNEUMATIC VALVE",
            ]

            for part in parts_to_analyze:
                part_data = df[df["ë¶€í’ˆëª…"].str.contains(part, case=False, na=False)]
                if len(part_data) > 0:
                    # ê°€ì¥ ë§ì€ ë¶ˆëŸ‰ìœ í˜• ì„ íƒ
                    most_common_defect = part_data["ëŒ€ë¶„ë¥˜"].value_counts().index[0]
                    defect_type_mapping[part] = most_common_defect
                    logger.info(f"ë™ì  ë§¤í•‘: {part} â†’ {most_common_defect}")

            # ê¸°ë³¸ê°’ë“¤ ì¶”ê°€
            default_mappings = {
                "HEATING JACKET": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "LEAK SENSOR": "ì „ì¥ì‘ì—…ë¶ˆëŸ‰",
                "TOUCH SCREEN": "ì „ì¥ì‘ì—…ë¶ˆëŸ‰",
                "REDUCER DOUBLE Y UNION": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "UNEQUAL UNION Y": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "CLAMP": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "MALE ELBOW": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "BULKHEAD UNION": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "KEY OPERATION VALVE": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "PNEUMATIC VALVE": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "ë¯¸ë¶„ë¥˜": "ê²€ì‚¬í’ˆì§ˆë¶ˆëŸ‰",
            }

            # ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ë¶€í’ˆë“¤ì— ëŒ€í•´ì„œë§Œ ê¸°ë³¸ê°’ ì ìš©
            for part, defect_type in default_mappings.items():
                if part not in defect_type_mapping:
                    defect_type_mapping[part] = defect_type

            logger.info(
                f"âœ… ë™ì  ë¶ˆëŸ‰ìœ í˜• ë§¤í•‘ ì™„ë£Œ: {len(defect_type_mapping)}ê°œ ë¶€í’ˆ"
            )
            return defect_type_mapping

        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì  ë¶ˆëŸ‰ìœ í˜• ë§¤í•‘ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©
            return {
                "HEATING JACKET": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "LEAK SENSOR": "ì „ì¥ì‘ì—…ë¶ˆëŸ‰",
                "TOUCH SCREEN": "ì „ì¥ì‘ì—…ë¶ˆëŸ‰",
                "FEMALE CONNECTOR": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "MALE CONNECTOR": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "SPEED CONTROLLER": "ë¶€í’ˆë¶ˆëŸ‰",
                "REDUCER DOUBLE Y UNION": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "UNEQUAL UNION Y": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "CLAMP": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "MALE ELBOW": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "BULKHEAD UNION": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "UNION ELBOW": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "KEY OPERATION VALVE": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "PNEUMATIC VALVE": "ê¸°êµ¬ì‘ì—…ë¶ˆëŸ‰",
                "BURNER SCRAPER LINE-01": "ë¶€í’ˆë¶ˆëŸ‰",
                "ë¯¸ë¶„ë¥˜": "ê²€ì‚¬í’ˆì§ˆë¶ˆëŸ‰",
            }

    def _get_actual_defect_count(self, part_name: str) -> int:
        """ì‹¤ì œ ë°ì´í„°ì—ì„œ ë¶€í’ˆë³„ ëˆ„ì  ë¶ˆëŸ‰ ê±´ìˆ˜ ì¡°íšŒ"""
        try:
            from data.teams_loader import TeamsDataLoader

            loader = TeamsDataLoader()
            df = loader.load_defect_data_from_teams()

            # í•´ë‹¹ ë¶€í’ˆì˜ ì‹¤ì œ ë¶ˆëŸ‰ ê±´ìˆ˜ ì¡°íšŒ
            part_data = df[df["ë¶€í’ˆëª…"].str.contains(part_name, case=False, na=False)]
            actual_count = len(part_data)

            logger.info(f"ì‹¤ì œ ëˆ„ì  ê±´ìˆ˜: {part_name} = {actual_count}ê±´")
            return actual_count if actual_count > 0 else 1

        except Exception as e:
            logger.warning(f"ì‹¤ì œ ëˆ„ì  ê±´ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return 5

    def create_advanced_dashboard_data(
        self, predictions: List[Dict], analysis: Dict, suggestions: List[Dict]
    ) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        logger.info("âœ… ê³ ë„í™”ëœ ë¶„ì„ ì™„ë£Œ!")

        # ë¶€í’ˆ ì„ ì • ê¸°ì¤€ ì„¤ëª… ì¶”ê°€
        selection_criteria = self._explain_part_selection_criteria(predictions)

        # ë™ì  ë¶ˆëŸ‰ìœ í˜• ë§¤í•‘ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        defect_type_mapping = self._get_dynamic_defect_type_mapping()

        # ê° ì˜ˆì¸¡ì— ë¶ˆëŸ‰ìœ í˜•ê³¼ ëˆ„ì ê±´ìˆ˜ ì¶”ê°€
        enhanced_predictions = []
        for pred in predictions:
            enhanced_pred = pred.copy()
            part_name = pred["ë¶€í’ˆ"]
            defect_rate = pred["ì˜ˆìƒë¶ˆëŸ‰ë¥ "]

            # ë¶ˆëŸ‰ìœ í˜• ì¶”ê°€
            enhanced_pred["ë¶ˆëŸ‰ìœ í˜•"] = defect_type_mapping.get(part_name, "ê¸°íƒ€ë¶ˆëŸ‰")

            # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ëˆ„ì ê±´ìˆ˜ ê³„ì‚°
            enhanced_pred["ëˆ„ì "] = self._get_actual_defect_count(part_name)

            # ê°œì„  ì œì•ˆ ì—°ê²°
            matching_suggestion = None
            for suggestion in suggestions:
                if part_name in suggestion.get("target", ""):
                    matching_suggestion = suggestion
                    break

            if matching_suggestion:
                enhanced_pred["ì œì•ˆ"] = (
                    matching_suggestion["specific_actions"][0]
                    if matching_suggestion["specific_actions"]
                    else "ê°œì„  ê²€í†  í•„ìš”"
                )
                enhanced_pred["ê·¼ê±°"] = (
                    f"ê·¼ê±°: {', '.join(matching_suggestion['root_causes'])}"
                )
                enhanced_pred["ì˜ˆìƒíš¨ê³¼"] = matching_suggestion["expected_improvement"]
                enhanced_pred["ìš°ì„ ìˆœìœ„"] = matching_suggestion["urgency"]
            else:
                enhanced_pred["ì œì•ˆ"] = "ê°œì„  ê²€í†  í•„ìš”"
                enhanced_pred["ê·¼ê±°"] = "ê·¼ê±°: ì¶”ê°€ ë¶„ì„ í•„ìš”"
                enhanced_pred["ì˜ˆìƒíš¨ê³¼"] = "íš¨ê³¼ ë¶„ì„ ì¤‘"
                enhanced_pred["ìš°ì„ ìˆœìœ„"] = "NORMAL"

            enhanced_predictions.append(enhanced_pred)

        # ìµœì¢… ëŒ€ì‹œë³´ë“œ ë°ì´í„°
        dashboard_data = {
            "predictions": enhanced_predictions,
            "advanced_analysis": analysis,
            "improvement_suggestions": suggestions,
            "selection_criteria": selection_criteria,  # ë¶€í’ˆ ì„ ì • ê¸°ì¤€ ì¶”ê°€
            "analysis_summary": self._generate_summary(analysis, enhanced_predictions),
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "analysis_version": "2.1",
        }

        logger.info(f"ğŸ” ëŒ€ì‹œë³´ë“œ ë°ì´í„° í‚¤: {list(dashboard_data.keys())}")
        if enhanced_predictions:
            logger.info(f"ğŸ” ì²« ë²ˆì§¸ ì˜ˆì¸¡ í‚¤: {list(enhanced_predictions[0].keys())}")

        if suggestions:
            first_suggestion = (
                suggestions[0]["specific_actions"][0]
                if suggestions[0]["specific_actions"]
                else "N/A"
            )
            logger.info(f"ğŸ” ê³ ë„í™”ëœ ì œì•ˆ: {first_suggestion}...")

        return dashboard_data

    def _explain_part_selection_criteria(
        self, predictions: List[Dict]
    ) -> Dict[str, Any]:
        """ë¶€í’ˆ ì„ ì • ê¸°ì¤€ ìƒì„¸ ì„¤ëª…"""
        return {
            "ranking_method": "ì˜ˆì¸¡ ë¶ˆëŸ‰ë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ",
            "primary_factors": [
                {
                    "factor": "ì˜ˆì¸¡ ë¶ˆëŸ‰ë¥ ",
                    "weight": "40%",
                    "description": "ML ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë¶€í’ˆë³„ ë¶ˆëŸ‰ ë°œìƒ í™•ë¥ ",
                    "current_range": (
                        f"{predictions[0]['ì˜ˆìƒë¶ˆëŸ‰ë¥ ']}% ~ {predictions[-1]['ì˜ˆìƒë¶ˆëŸ‰ë¥ ']}%"
                        if predictions
                        else "N/A"
                    ),
                },
                {
                    "factor": "ìƒì‚°ëŸ‰ ê°€ì¤‘ì¹˜",
                    "weight": "30%",
                    "description": "ì‹¤ì œ ì›”ê°„ ìƒì‚°ëŸ‰ì— ë”°ë¥¸ ì¤‘ìš”ë„ ë°˜ì˜",
                    "current_focus": "GAIA-I DUAL(34.7%), GAIA-I(32.4%)",
                },
                {
                    "factor": "ê³¼ê±° ë¶ˆëŸ‰ ë¹ˆë„",
                    "weight": "20%",
                    "description": "í•´ë‹¹ ë¶€í’ˆì˜ ì—­ì‚¬ì  ë¶ˆëŸ‰ ë°œìƒ ê±´ìˆ˜",
                    "data_period": "ìµœê·¼ 12ê°œì›” ë°ì´í„° ê¸°ì¤€",
                },
                {
                    "factor": "í‚¤ì›Œë“œ ìœ ì‚¬ë„",
                    "weight": "10%",
                    "description": "TF-IDF ê¸°ë°˜ ë¶ˆëŸ‰ ë‚´ìš© ìœ ì‚¬ì„± ë¶„ì„",
                    "method": "MeCab í˜•íƒœì†Œ ë¶„ì„ + í•œêµ­ì–´ ë¶ˆìš©ì–´ ì œê±°",
                },
            ],
            "special_cases": {
                "ë¯¸ë¶„ë¥˜": {
                    "current_status": f"{[p for p in predictions if p['ë¶€í’ˆ']=='ë¯¸ë¶„ë¥˜'][0]['ì˜ˆìƒë¶ˆëŸ‰ë¥ '] if any(p['ë¶€í’ˆ']=='ë¯¸ë¶„ë¥˜' for p in predictions) else 0}%",
                    "impact": "ì „ì²´ ë¶ˆëŸ‰ì˜ 26.7% ì°¨ì§€",
                    "priority": "HIGH - í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„ ê·¼ë³¸ì  ê°œì„  í•„ìš”",
                    "main_issue": "ì¡°ë¦½ë„ë©´/BOM ì •ë³´ ë¶ˆì¼ì¹˜ê°€ ì£¼ì›ì¸",
                }
            },
            "selection_logic": [
                "1. í•™ìŠµëœ ë°ì´í„°ì—ì„œ ìƒì‚°ëŸ‰ ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ìƒ˜í”Œë§",
                "2. ML ëª¨ë¸ë¡œ ê° ìƒ˜í”Œì˜ ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡",
                "3. ì˜ˆì¸¡ í™•ë¥  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬",
                "4. ìƒìœ„ 5ê°œ ë¶€í’ˆ ì„ ì •",
                "5. ê° ë¶€í’ˆë³„ ë§ì¶¤í˜• ê°œì„  ì œì•ˆ ë§¤ì¹­",
            ],
        }

    def _get_enhanced_keyword_analysis(self) -> Dict[str, Any]:
        """ê°œì„ ëœ í‚¤ì›Œë“œ ë¶„ì„ (ì˜ì–´+í•œêµ­ì–´ í†µí•©)"""
        try:
            from data.teams_loader import TeamsIntegratedDataLoader
            from data.data_loader import DataLoader
            
            # ë°ì´í„° ë¡œë“œ
            teams_loader = TeamsIntegratedDataLoader()
            data = teams_loader.load_data_with_fallback()
            data_loader = DataLoader()
            
            # ê°œì„ ëœ ì „ì²˜ë¦¬ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for content in data['ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©']:
                keywords = data_loader.preprocess_text(content)
                all_keywords.extend(keywords)
            
            keyword_freq = Counter(all_keywords).most_common(20)
            
            # í‚¤ì›Œë“œ ë¶„ë¥˜
            leak_keywords = [(kw, freq) for kw, freq in keyword_freq if 'leak' in kw.lower() or 'ëˆ„ìˆ˜' in kw or 'ëˆ„ì„¤' in kw]
            fitting_keywords = [(kw, freq) for kw, freq in keyword_freq if 'fitting' in kw.lower() or 'ì²´ê²°' in kw]
            material_keywords = [(kw, freq) for kw, freq in keyword_freq if any(mat in kw.lower() for mat in ['pfa', 'teflon', 'ìš°ë ˆíƒ„', 'ring'])]
            
            return {
                "total_keywords": len(set(all_keywords)),
                "top_keywords": keyword_freq,
                "categorized_analysis": {
                    "leak_related": leak_keywords,
                    "fitting_related": fitting_keywords, 
                    "material_related": material_keywords,
                },
                "enhanced_preprocessing": {
                    "english_keywords": len([kw for kw, _ in keyword_freq if kw[0].isupper() or any(c.isalpha() and c.isupper() for c in kw)]),
                    "korean_keywords": len([kw for kw, _ in keyword_freq if any('\uac00' <= c <= '\ud7a3' for c in kw)]),
                    "mixed_keywords": len([kw for kw, _ in keyword_freq if any('\uac00' <= c <= '\ud7a3' for c in kw) and any(c.isalpha() and c.isupper() for c in kw)]),
                }
            }
            
        except Exception as e:
            logger.warning(f"ê°œì„ ëœ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}

    def _generate_summary(
        self, analysis: Dict, predictions: List[Dict]
    ) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± (ê°œì„ ëœ í‚¤ì›Œë“œ ë¶„ì„ í¬í•¨)"""
        
        # ê°œì„ ëœ í‚¤ì›Œë“œ ë¶„ì„ ì¶”ê°€
        enhanced_keywords = self._get_enhanced_keyword_analysis()
        
        summary = {
            "total_predictions": len(predictions),
            "high_risk_parts": len([p for p in predictions if p.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0) >= 10]),
            "medium_risk_parts": len([p for p in predictions if 5 <= p.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0) < 10]),
            "low_risk_parts": len([p for p in predictions if p.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0) < 5]),
            "enhanced_keyword_analysis": enhanced_keywords,
            "data_quality_improvements": {
                "keyword_extraction": "ì˜ì–´+í•œêµ­ì–´ í†µí•© ì „ì²˜ë¦¬ ì ìš©",
                "accuracy_improvement": "+3.71%p (92.59% â†’ 96.30%)",
                "feature_enhancement": "59ê°œ â†’ 173ê°œ ì°¨ì› (+193.2%)",
                "keyword_density": "1.81ê°œ â†’ 5.34ê°œ (+195.2%)",
            },
            "top_improvement_areas": [
                "SPEED CONTROLLER (CRITICAL - 92ê±´ ëˆ„ìˆ˜)",
                "MALE ELBOW (HIGH - 40ê±´ Fitting nut ëˆ„ìˆ˜)",
                "MALE CONNECTOR (HIGH - 17ê±´ ëˆ„ìˆ˜)",
                "REDUCER DOUBLE Y UNION (HIGH - 41ê±´ ì‚½ì…ë¶€)",
            ]
        }
        
        if enhanced_keywords:
            summary["key_insights"] = {
                "most_critical_keyword": enhanced_keywords["top_keywords"][0] if enhanced_keywords["top_keywords"] else "N/A",
                "leak_dominance": len(enhanced_keywords.get("categorized_analysis", {}).get("leak_related", [])),
                "multilingual_coverage": f"ì˜ì–´ {enhanced_keywords.get('enhanced_preprocessing', {}).get('english_keywords', 0)}ê°œ, í•œêµ­ì–´ {enhanced_keywords.get('enhanced_preprocessing', {}).get('korean_keywords', 0)}ê°œ"
            }
        
        return summary
