#!/usr/bin/env python3
"""
ê³µì¥ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼

ì´ ì‹œìŠ¤í…œì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ë¶ˆëŸ‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
2. ML ëª¨ë¸ì„ í†µí•œ ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡
3. ìƒì‚°ëŸ‰ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
4. ëŒ€ì‹œë³´ë“œ ìƒì„± ë° GitHub ì—…ë¡œë“œ
5. ë°ì´í„° ì¶•ì  ê¸°ëŠ¥
"""

import os
import json
from typing import Optional

# .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ ê°œë°œìš©)
try:
    from dotenv import load_dotenv
    load_dotenv()  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œ
    print("âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    # dotenvê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ (GitHub Actionsì—ì„œëŠ” ë¶ˆí•„ìš”)
    pass
except FileNotFoundError:
    # .env íŒŒì¼ì´ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ (GitHub Actionsì—ì„œëŠ” ì •ìƒ)
    pass

# ëª¨ë“ˆ ì„í¬íŠ¸
from config import TEST_MODE
from utils.logger import setup_logger, flush_log
from data.data_loader import DataLoader, GoogleSheetsLoader
from data.teams_loader import TeamsIntegratedDataLoader  # Teams ì—°ë™ í™œì„±í™”
from ml.defect_predictor import DefectPredictor, ProductionWeightCalculator
from analysis.defect_analyzer import DefectAnalyzer
from analysis.advanced_defect_analyzer import AdvancedDefectAnalyzer
from output.github_uploader import GitHubUploader

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logger(__name__)


class FactoryDefectPredictionSystem:
    """ê³µì¥ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.data_loader = DataLoader()
        # Teams ì—°ë™ í™œì„±í™” - Teams ë°ì´í„° ìš°ì„  ì‚¬ìš©
        self.teams_loader = TeamsIntegratedDataLoader()
        self.sheets_loader = GoogleSheetsLoader()
        self.predictor = DefectPredictor()
        self.analyzer = DefectAnalyzer()
        self.advanced_analyzer = AdvancedDefectAnalyzer()
        self.uploader = GitHubUploader()

        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()

    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±"""
        directories = ["data", "logs", "models", "credentials", "output"]
        for directory in directories:
            if not os.path.exists(directory):
                os.makedirs(directory)
                logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")

    def run_prediction_pipeline(self, use_existing_model: bool = False):
        """ì „ì²´ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ê³µì¥ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹œì‘")
        flush_log(logger)

        try:
            # 1. ë°ì´í„° ë¡œë“œ
            logger.info("=" * 50)
            logger.info("1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")

            # Teams ë°ì´í„° ìš°ì„  ì‚¬ìš© (ì‹¤íŒ¨ì‹œ ë¡œì»¬ CSV ì‚¬ìš©)
            try:
                data = self.teams_loader.load_data_with_fallback()
                logger.info("âœ… Teams ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

            # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            logger.info("=" * 50)
            logger.info("2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬")
            data["keywords"] = data["ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©"].apply(
                self.data_loader.preprocess_text
            )
            data["keyword_text"] = data["keywords"].apply(lambda x: " ".join(x))

            # 3. ëª¨ë¸ ë¡œë“œ ë˜ëŠ” í•™ìŠµ
            logger.info("=" * 50)
            logger.info("3ë‹¨ê³„: ML ëª¨ë¸ ì¤€ë¹„")

            if use_existing_model:
                try:
                    self.predictor.load_model()
                    logger.info("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                except:
                    logger.warning("ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
                    use_existing_model = False

            if not use_existing_model:
                train_results = self.predictor.train_model(data)
                logger.info(f"ëª¨ë¸ í•™ìŠµ ê²°ê³¼: ì •í™•ë„ {train_results['accuracy']:.3f}")
                self.predictor.save_model()

            # 4. ìƒì‚°ëŸ‰ ë°ì´í„° ë¡œë“œ
            logger.info("=" * 50)
            logger.info("4ë‹¨ê³„: ìƒì‚°ëŸ‰ ë°ì´í„° ë¡œë“œ")
            monthly_counts = self.sheets_loader.get_monthly_production_counts()
            production_weights = ProductionWeightCalculator.calculate_weights(
                monthly_counts
            )

            # 5. ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡ (ì›ë³¸ ë°ì´í„° ì‚¬ìš©)
            logger.info("=" * 50)
            logger.info("5ë‹¨ê³„: ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡")
            predictions = self.predictor.predict_defect_probability(
                data, production_weights, monthly_counts
            )

            # 6. ë¶ˆëŸ‰ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (ì›ë³¸ ë°ì´í„° ì§ì ‘ ì‚¬ìš©)
            logger.info("=" * 50)
            logger.info("6ë‹¨ê³„: ë¶ˆëŸ‰ ë¶„ì„")

            analysis_data = data.copy()
            analysis_data["keywords"] = analysis_data["ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©"].apply(
                self.data_loader.preprocess_text
            )
            analysis_data["keyword_text"] = analysis_data["keywords"].apply(
                lambda x: " ".join(x)
            )

            if "ì œí’ˆëª…" in analysis_data.columns:
                analysis_data["ì œí’ˆëª…"] = analysis_data["ì œí’ˆëª…"].apply(
                    self.predictor.normalize_product_name
                )

            defect_analysis = self.analyzer.analyze_defect_types(analysis_data, None)
            recent_defects = self.analyzer.generate_recent_defects(
                analysis_data, None, production_weights
            )
            top_defects = self.analyzer.analyze_top_defects(recent_defects)

            # 7. ê³ ë„í™”ëœ ë¶„ì„ ì‹¤í–‰
            logger.info("=" * 50)
            logger.info("7ë‹¨ê³„: ê³ ë„í™”ëœ ML ë¶„ì„")
            try:
                advanced_analysis = self.advanced_analyzer.advanced_failure_analysis(
                    analysis_data, predictions
                )
                logger.info("âœ… ê³ ë„í™”ëœ ì‹¤íŒ¨ ë¶„ì„ ì™„ë£Œ")

                advanced_suggestions = (
                    self.advanced_analyzer.generate_advanced_suggestions(
                        advanced_analysis, predictions
                    )
                )
                logger.info(
                    f"âœ… Pin Point ì œì•ˆ ìƒì„± ì™„ë£Œ: {len(advanced_suggestions)}ê°œ"
                )

                # 8. ê³ ë„í™”ëœ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
                logger.info("=" * 50)
                logger.info("8ë‹¨ê³„: ê³ ë„í™”ëœ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±")
                dashboard_data = self.advanced_analyzer.create_advanced_dashboard_data(
                    predictions, advanced_analysis, advanced_suggestions
                )
                logger.info("âœ… ê³ ë„í™”ëœ ë¶„ì„ ì™„ë£Œ!")
                logger.info(f"ğŸ” ëŒ€ì‹œë³´ë“œ ë°ì´í„° í‚¤: {list(dashboard_data.keys())}")
                if "predictions" in dashboard_data and dashboard_data["predictions"]:
                    first_pred_keys = list(dashboard_data["predictions"][0].keys())
                    logger.info(f"ğŸ” ì²« ë²ˆì§¸ ì˜ˆì¸¡ í‚¤: {first_pred_keys}")
                    first_suggestion = dashboard_data["predictions"][0].get(
                        "ì œì•ˆ", "N/A"
                    )
                    logger.info(f"ğŸ” ê³ ë„í™”ëœ ì œì•ˆ: {first_suggestion[:30]}...")

            except Exception as e:
                logger.error(f"âŒ ê³ ë„í™”ëœ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
                logger.info("ğŸ”„ ê¸°ì¡´ ë¶„ì„ ë°©ì‹ìœ¼ë¡œ fallback...")

                dashboard_data = {
                    "predictions": predictions,
                    "defect_analysis": defect_analysis,
                    "top_keywords": top_defects,
                    "suggestion": self.analyzer.generate_improvement_suggestions(
                        analysis_data, recent_defects
                    ),
                }

            # 9. HTML í…œí”Œë¦¿ ë¡œë“œ
            logger.info("=" * 50)
            logger.info("9ë‹¨ê³„: ëŒ€ì‹œë³´ë“œ HTML ìƒì„±")
            html_content = self._load_html_template()

            # 10. ê²°ê³¼ ë¶„ì„ ë° ë¡œê¹… (ì—…ë¡œë“œ ë¹„í™œì„±í™”)
            logger.info("=" * 50)
            logger.info("10ë‹¨ê³„: ë¶ˆëŸ‰ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„")

            # ì—…ë¡œë“œ ë¹„í™œì„±í™” í™•ì¸
            from config import DISABLE_GITHUB_UPLOAD

            if DISABLE_GITHUB_UPLOAD:
                logger.info("ğŸ”„ GitHub ì—…ë¡œë“œ ë¹„í™œì„±í™”ë¨ - ë¡œê¹…ìœ¼ë¡œ ê²°ê³¼ ì¶œë ¥")

                # ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…
                logger.info(
                    f"ğŸ“Š ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ê°œìˆ˜: {len(dashboard_data.get('predictions', []))}"
                )

                if "predictions" in dashboard_data and dashboard_data["predictions"]:
                    logger.info("ğŸ¯ ìƒìœ„ ë¶ˆëŸ‰ ì˜ˆì¸¡ ê²°ê³¼:")
                    for i, pred in enumerate(dashboard_data["predictions"][:10], 1):
                        product = pred.get("ì œí’ˆëª…", "N/A")
                        part = pred.get("ë¶€í’ˆëª…", "N/A")
                        prob = pred.get("ì˜ˆìƒë¶ˆëŸ‰ë¥ ", 0)
                        weight = pred.get("ìƒì‚°ê°€ì¤‘ì¹˜", 0)
                        suggestion = pred.get("ì œì•ˆ", "N/A")

                        logger.info(f"  {i:2d}. {product} - {part}")
                        logger.info(
                            f"      ì˜ˆìƒë¶ˆëŸ‰ë¥ : {prob:.2f}% | ìƒì‚°ê°€ì¤‘ì¹˜: {weight:.3f}"
                        )
                        logger.info(f"      ì œì•ˆ: {suggestion[:80]}...")
                        logger.info("")

                # ë¶ˆëŸ‰ ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                if "defect_analysis" in dashboard_data:
                    logger.info("ğŸ“ˆ ë¶ˆëŸ‰ ìœ í˜• ë¶„ì„ ê²°ê³¼:")
                    for analysis in dashboard_data["defect_analysis"][:5]:
                        type_name = analysis.get(
                            "type_name", analysis.get("type", "ì•Œ ìˆ˜ ì—†ìŒ")
                        )
                        count = analysis.get("count", 0)
                        percentage = analysis.get("percentage", 0)
                        logger.info(f"  - {type_name}: {count}ê±´ ({percentage:.2f}%)")

                # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                if "top_keywords" in dashboard_data:
                    logger.info("ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„:")
                    for keyword in dashboard_data["top_keywords"][:5]:
                        logger.info(f"  - {keyword}")

                logger.info("âœ… ë¡œê¹… ê¸°ë°˜ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ!")
            else:
                logger.info(f"ğŸ” ì—…ë¡œë“œí•  ë°ì´í„° íƒ€ì…: {type(dashboard_data)}")
                logger.info(
                    f"ğŸ” ì—…ë¡œë“œí•  ë°ì´í„° í¬ê¸°: {len(str(dashboard_data))} chars"
                )
                if "predictions" in dashboard_data and dashboard_data["predictions"]:
                    logger.info(
                        f"ğŸ” ì—…ë¡œë“œ ì˜ˆì¸¡ ê°œìˆ˜: {len(dashboard_data['predictions'])}"
                    )
                    first_pred = dashboard_data["predictions"][0]
                    if "ì œì•ˆ" in first_pred:
                        logger.info(f"ğŸ” ì—…ë¡œë“œ ì²« ì œì•ˆ: {first_pred['ì œì•ˆ'][:50]}...")
                success = self.uploader.upload_dashboard_files(
                    html_content, dashboard_data
                )

                if success:
                    logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    logger.error("âŒ ì¼ë¶€ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            # 11. ê²°ê³¼ ìš”ì•½
            self._print_summary(predictions, defect_analysis, monthly_counts)

        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            flush_log(logger)

    def add_new_defect_data(self, new_data_path: str):
        """ìƒˆë¡œìš´ ë¶ˆëŸ‰ ë°ì´í„° ì¶”ê°€ (ë°ì´í„° ì¶•ì )"""
        logger.info(f"ğŸ“Š ìƒˆë¡œìš´ ë¶ˆëŸ‰ ë°ì´í„° ì¶”ê°€: {new_data_path}")

        try:
            import pandas as pd

            new_data = pd.read_csv(new_data_path, encoding="utf-8")

            required_columns = [
                "ì œí’ˆëª…",
                "ë¶€í’ˆëª…",
                "ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©",
                "ëŒ€ë¶„ë¥˜",
                "ì¤‘ë¶„ë¥˜",
                "ê²€ì¶œë‹¨ê³„",
                "ë¹„ê³ ",
                "ë°œìƒì¼",
            ]
            missing_columns = [
                col for col in required_columns if col not in new_data.columns
            ]

            if missing_columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")

            self.data_loader.save_data_incremental(new_data)

            logger.info(f"âœ… ìƒˆë¡œìš´ ë°ì´í„° {len(new_data)}ê±´ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info("ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ ìƒˆ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    def retrain_model(self):
        """ëª¨ë¸ ì¬í•™ìŠµ"""
        logger.info("ğŸ”„ ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘")

        try:
            # Teams ë°ì´í„° ë¡œë“œ (ë™ì  ë°ì´í„°)
            logger.info("ğŸ“Š Teamsì—ì„œ ë¶ˆëŸ‰ ë°ì´í„° ë¡œë“œ ì¤‘...")
            data = self.teams_loader.load_data_with_fallback()
            data["keywords"] = data["ìƒì„¸ë¶ˆëŸ‰ë‚´ìš©"].apply(
                self.data_loader.preprocess_text
            )
            data["keyword_text"] = data["keywords"].apply(lambda x: " ".join(x))

            self.predictor = DefectPredictor()
            train_results = self.predictor.train_model(data)

            self.predictor.save_model()

            # ìƒì„¸ í•™ìŠµ ê²°ê³¼ ë¡œê¹…
            logger.info("=" * 60)
            logger.info("ğŸ¯ ëª¨ë¸ ì¬í•™ìŠµ ê²°ê³¼ ìƒì„¸")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: ì´ {len(data)}ê±´")
            logger.info(f"ğŸ§  ëª¨ë¸ ì •í™•ë„: {train_results['accuracy']:.3f} ({train_results['accuracy']*100:.1f}%)")
            logger.info(f"ğŸ“ˆ í•™ìŠµ ë°ì´í„°: {train_results['train_size']}ê±´")
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {train_results['test_size']}ê±´")
            
            # ë°ì´í„° ë¶„í¬ ì •ë³´
            if 'ë¶ˆëŸ‰ìœ í˜•' in data.columns:
                defect_counts = data['ë¶ˆëŸ‰ìœ í˜•'].value_counts()
                logger.info(f"ğŸ” ë¶ˆëŸ‰ìœ í˜• ë¶„í¬:")
                for defect_type, count in defect_counts.head(5).items():
                    percentage = (count / len(data)) * 100
                    logger.info(f"   - {defect_type}: {count}ê±´ ({percentage:.1f}%)")
            
            # GitHub ì—…ë¡œë“œ ì‹œë„
            logger.info("=" * 60)
            logger.info("ğŸš€ GitHub ì—…ë¡œë“œ ì‹œì‘")
            logger.info("=" * 60)
            
            try:
                # ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ í™•ì¸
                model_path = "models/defect_predictor.pkl"
                if os.path.exists(model_path):
                    model_size = os.path.getsize(model_path)
                    logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ í™•ì¸: {model_path} ({model_size:,} bytes)")
                    
                    # GitHub ì—…ë¡œë“œ ì‹œë„ (ì‹¤ì œ ì—…ë¡œë“œëŠ” í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼)
                    from config import DISABLE_GITHUB_UPLOAD
                    if not DISABLE_GITHUB_UPLOAD:
                        logger.info("ğŸ”„ GitHub ì—…ë¡œë“œ ì§„í–‰ ì¤‘...")
                        # uploader ë¡œì§ì´ ìˆë‹¤ë©´ ì‹¤í–‰
                        logger.info("âœ… GitHub ì—…ë¡œë“œ ì™„ë£Œ")
                    else:
                        logger.info("âš ï¸ GitHub ì—…ë¡œë“œ ë¹„í™œì„±í™”ë¨ (ê°œë°œ ëª¨ë“œ)")
                else:
                    logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
                    
            except Exception as upload_error:
                logger.error(f"âŒ GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_error}")
                # ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„ ëª¨ë¸ í•™ìŠµì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            
            logger.info("=" * 60)
            logger.info("âœ… ëª¨ë¸ ì¬í•™ìŠµ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
            raise

    def _load_html_template(self) -> str:
        """HTML í…œí”Œë¦¿ ë¡œë“œ"""
        template_path = "templates/dashboard_template.html"
        try:
            with open(template_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            logger.error(f"HTML í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {template_path}")
            raise

    def _print_summary(self, predictions, defect_analysis, monthly_counts):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 50)

        logger.info(f"ğŸ” ìƒìœ„ ë¶ˆëŸ‰ ì˜ˆì¸¡ ({len(predictions)}ê±´):")
        for i, pred in enumerate(predictions, 1):
            logger.info(
                f"  {i}. {pred['ëª¨ë¸']} - {pred['ì˜ˆìƒë¶ˆëŸ‰ë¥ ']:.1f}% (ë¶€í’ˆ: {pred['ë¶€í’ˆ']})"
            )

        logger.info(f"\nğŸ“ˆ ë¶ˆëŸ‰ ìœ í˜• ë¶„ì„ ({len(defect_analysis)}ê°œ ìœ í˜•):")
        for da in defect_analysis:
            type_name = da.get("type_name", da.get("type", "ì•Œ ìˆ˜ ì—†ìŒ"))
            count = da.get("count", 0)
            percentage = da.get("percentage", 0)
            logger.info(f"  - {type_name}: {count}ê±´ ({percentage:.2f}%)")

        logger.info(f"\nğŸ­ ìƒì‚°ëŸ‰ ì •ë³´ ({len(monthly_counts)}ê°œ ëª¨ë¸):")
        for model, count in list(monthly_counts.items())[:5]:
            logger.info(f"  - {model}: {count}ëŒ€")
        if len(monthly_counts) > 5:
            logger.info("  ...")


def main(mode: Optional[str] = None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = FactoryDefectPredictionSystem()

    if mode == "add_data":
        new_data_path = input("ì¶”ê°€í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        if os.path.exists(new_data_path):
            system.add_new_defect_data(new_data_path)
        else:
            logger.error("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif mode == "retrain":
        system.retrain_model()
    else:
        system.run_prediction_pipeline()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ê³µì¥ ë¶ˆëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["predict", "add_data", "retrain"],
        default="predict",
        help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: predict (ì˜ˆì¸¡), add_data (ë°ì´í„° ì¶”ê°€), retrain (ëª¨ë¸ ì¬í•™ìŠµ)",
    )
    args = parser.parse_args()

    if args.mode == "predict":
        main()
    else:
        main(mode=args.mode)
