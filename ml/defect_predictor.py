import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
from typing import Dict, List, Tuple, Any
import pickle
import os
from collections import defaultdict

from config import ml_config
from utils.logger import setup_logger, flush_log

logger = setup_logger(__name__)


class DefectPredictor:
    """ë¶ˆëŸ‰ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.model = None
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.feature_names = None
        self.is_trained = False

        # ì œí’ˆëª… ë§¤í•‘ (CSV ì œí’ˆëª… -> ìƒì‚°ëŸ‰ ë°ì´í„° ì œí’ˆëª…)
        self.product_name_mapping = {
            "DRAGON AB DUAL": "DRAGON DUAL",
            "DRAGON AB": "DRAGON",
            # í•„ìš”ì‹œ ì¶”ê°€ ë§¤í•‘ ê·œì¹™
        }

    def normalize_product_name(self, product_name: str) -> str:
        """ì œí’ˆëª…ì„ ìƒì‚°ëŸ‰ ë°ì´í„°ì™€ ë§¤ì¹­ë˜ë„ë¡ ì •ê·œí™”"""
        return self.product_name_mapping.get(product_name, product_name)

    def prepare_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """í”¼ì²˜ ì¤€ë¹„ ë° íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
        logger.info("ğŸ”§ í”¼ì²˜ ì¤€ë¹„ ì¤‘...")

        # ì œí’ˆëª… ì •ê·œí™”
        if "ì œí’ˆëª…" in data.columns:
            data["ì œí’ˆëª…"] = data["ì œí’ˆëª…"].apply(self.normalize_product_name)
            logger.info(f"ì œí’ˆëª… ì •ê·œí™” ì™„ë£Œ: {data['ì œí’ˆëª…'].unique()}")

        # ë¼ë²¨ ì¸ì½”ë”©
        columns_to_encode = ["ì œí’ˆëª…", "ë¶€í’ˆëª…", "ê²€ì¶œë‹¨ê³„", "ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜"]

        for column in columns_to_encode:
            # NaN ê°’ì„ "ë¯¸ë¶„ë¥˜"ë¡œ ëŒ€ì²´
            data[column] = data[column].fillna("ë¯¸ë¶„ë¥˜")

            if column not in self.label_encoders:
                self.label_encoders[column] = LabelEncoder()
                data[column] = self.label_encoders[column].fit_transform(data[column])
            else:
                # ìƒˆë¡œìš´ ë¼ë²¨ì´ ìˆì„ ê²½ìš° ì²˜ë¦¬
                try:
                    data[column] = self.label_encoders[column].transform(data[column])
                except ValueError as e:
                    logger.warning(f"ìƒˆë¡œìš´ ë¼ë²¨ ë°œê²¬ ({column}): {e}")
                    # ìƒˆë¡œìš´ ë¼ë²¨ì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ í•™ìŠµ
                    self.label_encoders[column] = LabelEncoder()
                    data[column] = self.label_encoders[column].fit_transform(
                        data[column]
                    )

        # TF-IDF ë²¡í„°í™”
        if self.tfidf_vectorizer is None:
            logger.info("ğŸ“ˆ TF-IDF ë²¡í„°í™” ì¤‘...")
            self.tfidf_vectorizer = TfidfVectorizer(
                max_df=ml_config.max_df, min_df=ml_config.min_df
            )
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(data["keyword_text"])
            self.feature_names = self.tfidf_vectorizer.get_feature_names_out()
        else:
            tfidf_matrix = self.tfidf_vectorizer.transform(data["keyword_text"])

        # ìˆ«ìí˜• í”¼ì²˜ì™€ TF-IDF í”¼ì²˜ ê²°í•©
        X_numeric = data[["ì œí’ˆëª…", "ë¶€í’ˆëª…", "ê²€ì¶œë‹¨ê³„"]].values
        X_tfidf = tfidf_matrix.toarray()
        X = np.hstack((X_numeric, X_tfidf))

        # íƒ€ê²Ÿ ë³€ìˆ˜ (ë¶€í’ˆë¶ˆëŸ‰ ì—¬ë¶€)
        try:
            target_label = self.label_encoders["ëŒ€ë¶„ë¥˜"].transform(["ë¶€í’ˆë¶ˆëŸ‰"])[0]
            y = (data["ëŒ€ë¶„ë¥˜"] == target_label).astype(int)
        except ValueError:
            # 'ë¶€í’ˆë¶ˆëŸ‰' ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš° ê°€ì¥ ë¹ˆë²ˆí•œ ë¼ë²¨ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
            most_common_label = data["ëŒ€ë¶„ë¥˜"].mode()[0]
            y = (data["ëŒ€ë¶„ë¥˜"] == most_common_label).astype(int)
            logger.warning(
                f"'ë¶€í’ˆë¶ˆëŸ‰' ë¼ë²¨ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ '{most_common_label}'ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
            )

        logger.info(f"âœ… í”¼ì²˜ ì¤€ë¹„ ì™„ë£Œ: {X.shape}, íƒ€ê²Ÿ ë¶„í¬: {np.bincount(y)}")
        flush_log(logger)

        return X, y

    def train_model(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("ğŸ§  ML ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # í”¼ì²˜ ì¤€ë¹„
        X, y = self.prepare_features(data)

        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        # ë™ì  random_state: ë§¤ë²ˆ ë‹¤ë¥¸ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘í•œ ì¸ì‚¬ì´íŠ¸ í™•ë³´
        current_random_state = ml_config.random_state
        if current_random_state is None:
            import time

            current_random_state = int(time.time()) % 10000
            logger.info(f"ğŸ² ë™ì  random_state ì‚¬ìš©: {current_random_state}")

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=ml_config.test_size,
            random_state=current_random_state,
            stratify=y if len(np.unique(y)) > 1 else None,
        )

        # ëª¨ë¸ í•™ìŠµ
        self.model = RandomForestClassifier(
            random_state=current_random_state, class_weight="balanced", n_estimators=100
        )
        self.model.fit(X_train, y_train)

        # ëª¨ë¸ í‰ê°€
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ - ì •í™•ë„: {accuracy:.3f}")
        logger.info(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}")

        # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = self.get_feature_importance()

        self.is_trained = True
        flush_log(logger)

        return {
            "accuracy": accuracy,
            "train_size": len(X_train),
            "test_size": len(X_test),
            "feature_importance": feature_importance,
        }

    def predict_defect_probability(
        self,
        data: pd.DataFrame,
        production_weights: Dict[str, float] = None,
        monthly_production_counts: Dict[str, int] = None,
    ) -> List[Dict]:
        """ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡ (ì›”ê°„ ìƒì‚° ëª¨ë¸ ì „ì²´ í‘œì‹œ)"""
        if not self.is_trained:
            raise ValueError(
                "ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
            )

        logger.info("ğŸ” ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡ ì¤‘...")

        # ì›”ê°„ ìƒì‚° ì¹´ìš´íŠ¸ ì„¤ì •
        if monthly_production_counts:
            self.monthly_production_counts = monthly_production_counts

        # ê¸°ì¡´ ì½”ë“œì²˜ëŸ¼: í•™ìŠµëœ ë°ì´í„°ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©
        # ì¦‰, ì´ë¯¸ ë¼ë²¨ ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        data_copy = data.copy()

        # ìƒì‚°ëŸ‰ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê¸°ì¡´ ì½”ë“œ ë°©ì‹)
        if production_weights:
            # ì´ë¯¸ ì¸ì½”ë”©ëœ ì œí’ˆëª…ì„ ì›ë˜ ì´ë¦„ìœ¼ë¡œ ë³µì›í•˜ì—¬ ê°€ì¤‘ì¹˜ ë§¤í•‘
            data_copy["production_weight"] = data_copy["ì œí’ˆëª…"].apply(
                lambda x: self._get_production_weight(x, production_weights)
            )
        else:
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            data_copy["production_weight"] = 1.0

        # ê°œì„ ëœ ìƒ˜í”Œë§: ì›”ê°„ ìƒì‚° ëª¨ë¸ ì „ì²´ í‘œì‹œ (SPEED CONTROLLER ì œì™¸)

        # SPEED CONTROLLER ì œì™¸ í•„í„°ë§
        speed_controller_encoded = None
        try:
            speed_controller_encoded = self.label_encoders["ë¶€í’ˆëª…"].transform(
                ["SPEED CONTROLLER"]
            )[0]
        except (ValueError, KeyError):
            pass

        # ì›”ê°„ ìƒì‚° ëª¨ë¸ë³„ ëŒ€í‘œ ë¶€í’ˆ ì„ ë³„ (SPEED CONTROLLER ì œì™¸)
        model_representative_samples = []

        # ì›”ê°„ ìƒì‚° ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        monthly_production_models = set()
        if (
            hasattr(self, "monthly_production_counts")
            and self.monthly_production_counts
        ):
            monthly_production_models = set(self.monthly_production_counts.keys())

        # ê° ì›”ê°„ ìƒì‚° ëª¨ë¸ë³„ë¡œ ë¶€í’ˆ ì„ ë³„
        for model_name in monthly_production_models:
            try:
                model_encoded = self.label_encoders["ì œí’ˆëª…"].transform([model_name])[0]
                model_data = data_copy[data_copy["ì œí’ˆëª…"] == model_encoded]

                # SPEED CONTROLLER ì œì™¸
                if speed_controller_encoded is not None:
                    model_data = model_data[
                        model_data["ë¶€í’ˆëª…"] != speed_controller_encoded
                    ]

                if len(model_data) == 0:
                    continue

                # í•´ë‹¹ ëª¨ë¸ì˜ ë¶€í’ˆë³„ ì¹´ìš´íŠ¸ ê³„ì‚°
                model_part_counts = (
                    model_data.groupby("ë¶€í’ˆëª…")
                    .size()
                    .reset_index(name="model_part_count")
                )
                model_data_with_counts = model_data.merge(
                    model_part_counts, on="ë¶€í’ˆëª…", how="left"
                )

                # ëª¨ë¸ë³„ ìƒìœ„ 2ê°œ ë¶€í’ˆ ì„ ë³„ (ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´)
                top_parts = (
                    model_data_with_counts.groupby("ë¶€í’ˆëª…")["model_part_count"]
                    .first()
                    .nlargest(2)
                    .index
                )

                for part_encoded in top_parts:
                    part_data = model_data_with_counts[
                        model_data_with_counts["ë¶€í’ˆëª…"] == part_encoded
                    ]
                    if len(part_data) > 0:
                        # í•´ë‹¹ ëª¨ë¸-ë¶€í’ˆ ì¡°í•©ì—ì„œ ëŒ€í‘œ ìƒ˜í”Œ ì„ íƒ
                        representative_sample = part_data.iloc[0:1].copy()
                        representative_sample["diversity_weight"] = part_data[
                            "model_part_count"
                        ].iloc[0]
                        representative_sample["production_count"] = (
                            self.monthly_production_counts.get(model_name, 0)
                        )
                        model_representative_samples.append(representative_sample)

            except (ValueError, KeyError) as e:
                logger.warning(f"âš ï¸ ëª¨ë¸ '{model_name}' ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                continue

        # ëŒ€í‘œ ìƒ˜í”Œë“¤ ê²°í•© ë° ìƒì‚°ëŸ‰ ê¸°ì¤€ ì •ë ¬
        if model_representative_samples:
            sample_data = pd.concat(model_representative_samples, ignore_index=True)
            # ìƒì‚°ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            sample_data = sample_data.sort_values(
                "production_count", ascending=False
            ).reset_index(drop=True)
        else:
            # ëŒ€ì²´ ë°©ì•ˆ: ê¸°ì¡´ ë°©ì‹
            sample_size = min(ml_config.sample_size, len(data_copy))
            # ë™ì  random_state ì‚¬ìš©
            sample_random_state = ml_config.random_state
            if sample_random_state is None:
                import time

                sample_random_state = int(time.time()) % 10000

            sample_data = data_copy.sample(
                n=sample_size,
                weights="production_weight",
                random_state=sample_random_state,
            ).reset_index(drop=True)

        # ë¡œê·¸: ìƒ˜í”Œë§ëœ ëª¨ë¸ ë¶„í¬
        sampled_models = sample_data["ì œí’ˆëª…"].apply(
            lambda x: self.label_encoders["ì œí’ˆëª…"].inverse_transform([x])[0]
        )
        sampled_model_counts = sampled_models.value_counts()

        logger.info(f"ìƒ˜í”Œë§ëœ ëª¨ë¸ ë¶„í¬: {sampled_model_counts.to_dict()}")
        logger.info(f"ì›”ê°„ ìƒì‚° ëª¨ë¸ ì „ì²´ í‘œì‹œ: {len(sampled_model_counts)}ê°œ ëª¨ë¸")

        # í”¼ì²˜ ì¤€ë¹„
        X_sample_numeric = sample_data[["ì œí’ˆëª…", "ë¶€í’ˆëª…", "ê²€ì¶œë‹¨ê³„"]].values
        X_sample_tfidf = self.tfidf_vectorizer.transform(
            sample_data["keyword_text"]
        ).toarray()
        X_sample = np.hstack((X_sample_numeric, X_sample_tfidf))

        # ì˜ˆì¸¡
        probs = self.model.predict_proba(X_sample)[:, 1] * 100

        # ê²°ê³¼ êµ¬ì„±
        predictions = []
        for i, (idx, row) in enumerate(sample_data.iterrows()):
            # ë¶€í’ˆëª… ì•ˆì „í•˜ê²Œ ë³€í™˜ (NaN ì²˜ë¦¬)
            try:
                part_name = self.label_encoders["ë¶€í’ˆëª…"].inverse_transform(
                    [row["ë¶€í’ˆëª…"]]
                )[0]
                if pd.isna(part_name) or part_name == "nan":
                    part_name = "ë¯¸ë¶„ë¥˜"
            except (ValueError, IndexError):
                part_name = "ë¯¸ë¶„ë¥˜"

            predictions.append(
                {
                    "ëª¨ë¸": self.label_encoders["ì œí’ˆëª…"].inverse_transform(
                        [row["ì œí’ˆëª…"]]
                    )[0],
                    "ê²€ì¶œë‹¨ê³„": self.label_encoders["ê²€ì¶œë‹¨ê³„"].inverse_transform(
                        [row["ê²€ì¶œë‹¨ê³„"]]
                    )[0],
                    "ë¶€í’ˆ": part_name,
                    "ì˜ˆìƒë¶ˆëŸ‰ë¥ ": round(float(probs[i]), 1),  # ì†Œìˆ˜ì  1ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                    "í‚¤ì›Œë“œ": ", ".join(row["keywords"]),
                }
            )

        # ë¶ˆëŸ‰ë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        top_predictions = sorted(
            predictions, key=lambda x: x["ì˜ˆìƒë¶ˆëŸ‰ë¥ "], reverse=True
        )[: ml_config.top_predictions_count]

        for pred in top_predictions:
            logger.info(
                f"- {pred['ëª¨ë¸']} ({pred['ê²€ì¶œë‹¨ê³„']}), ë¶€í’ˆ: {pred['ë¶€í’ˆ']}, ë¶ˆëŸ‰ í™•ë¥ : {pred['ì˜ˆìƒë¶ˆëŸ‰ë¥ ']:.1f}%"
            )

        flush_log(logger)
        return top_predictions

    def _get_production_weight(
        self, encoded_product: int, production_weights: Dict[str, float]
    ) -> float:
        """ì¸ì½”ë”©ëœ ì œí’ˆëª…ì„ ì›ë˜ ì´ë¦„ìœ¼ë¡œ ë³µì›í•˜ì—¬ ìƒì‚°ëŸ‰ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        try:
            product_name = self.label_encoders["ì œí’ˆëª…"].inverse_transform(
                [encoded_product]
            )[0]
            return production_weights.get(product_name, 0.01)  # ê¸°ë³¸ê°’
        except Exception:
            return 0.01

    def get_feature_importance(self) -> List[Tuple[str, float]]:
        """í”¼ì²˜ ì¤‘ìš”ë„ ë°˜í™˜"""
        if not self.is_trained:
            return []

        # ìˆ«ìí˜• í”¼ì²˜ ì´ë¦„
        numeric_features = ["ì œí’ˆëª…", "ë¶€í’ˆëª…", "ê²€ì¶œë‹¨ê³„"]

        # ëª¨ë“  í”¼ì²˜ ì´ë¦„
        all_features = numeric_features + list(self.feature_names)

        # ì¤‘ìš”ë„ ê³„ì‚°
        importances = self.model.feature_importances_
        feature_importance = list(zip(all_features, importances))

        # ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        feature_importance.sort(key=lambda x: x[1], reverse=True)

        return feature_importance[:20]  # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜

    def get_top_keywords(self) -> List[Tuple[str, float]]:
        """TF-IDF ê¸°ì¤€ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜"""
        if self.tfidf_vectorizer is None:
            return []

        # TF-IDF ì ìˆ˜ ê³„ì‚°
        tfidf_scores = np.sum(
            self.tfidf_vectorizer.transform(["dummy"]).toarray(), axis=0
        )
        top_keywords = sorted(
            zip(self.feature_names, tfidf_scores), key=lambda x: x[1], reverse=True
        )[: ml_config.top_keywords_count]

        return top_keywords

    def save_model(self, file_path: str = "models/defect_predictor.pkl"):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        model_data = {
            "model": self.model,
            "label_encoders": self.label_encoders,
            "tfidf_vectorizer": self.tfidf_vectorizer,
            "feature_names": self.feature_names,
            "is_trained": self.is_trained,
        }

        with open(file_path, "wb") as f:
            pickle.dump(model_data, f)

        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {file_path}")
        flush_log(logger)

    def load_model(self, file_path: str = "models/defect_predictor.pkl"):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(file_path, "rb") as f:
                model_data = pickle.load(f)

            self.model = model_data["model"]
            self.label_encoders = model_data["label_encoders"]
            self.tfidf_vectorizer = model_data["tfidf_vectorizer"]
            self.feature_names = model_data["feature_names"]
            self.is_trained = model_data["is_trained"]

            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {file_path}")
            flush_log(logger)

        except FileNotFoundError:
            logger.warning(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise


class ProductionWeightCalculator:
    """ìƒì‚°ëŸ‰ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°ê¸°"""

    @staticmethod
    def calculate_weights(
        monthly_production_counts: Dict[str, int],
    ) -> Dict[str, float]:
        """ìƒì‚°ëŸ‰ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        logger.info("ğŸ“ˆ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")

        if not monthly_production_counts:
            logger.warning("ìƒì‚°ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        total_production = sum(monthly_production_counts.values())

        # ì›ì‹œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        raw_weights = {
            model: count / total_production
            for model, count in monthly_production_counts.items()
        }

        # ìµœì†Œ/ìµœëŒ€ ê°€ì¤‘ì¹˜ ì ìš©
        production_weights = {
            model: min(max(weight, ml_config.min_weight), ml_config.max_weight)
            for model, weight in raw_weights.items()
        }

        # ì •ê·œí™”
        total_weight = sum(production_weights.values())
        production_weights = {
            model: weight / total_weight for model, weight in production_weights.items()
        }

        logger.info(f"ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {production_weights}")
        flush_log(logger)

        return production_weights
